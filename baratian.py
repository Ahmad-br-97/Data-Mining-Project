# -*- coding: utf-8 -*-
"""baratian.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wEgqtSnDREwkfP_2tDC3-rUi1t5wfOQg
"""

!pip install stopwords_guilannlp

!pip install hazm

!pip install keras
!pip install nltk

import nltk
nltk.download('punkt')

import csv
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords
from hazm import *
import re
from stopwords_guilannlp import *
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report


STOPWORDS = stopwords_output("Persian","nar")

vocab_size = 500
embedding_dim = 64
max_length = 250
trunc_type = 'post'
padding_type = 'post'
oov_tok = '<OOV>'
training_portion = .8

articles = []
labels = []

datas=pd.read_excel("/content/My_DigiKala_Comments_1.xlsx")
for text in datas['comment']:
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) 
data_classes=['U0001F636', 'U0001F630', 'U0001F9D0', 'U0001F642', 'U0001F648', 'U0001F61B', 'U0001F915', 'U0001F493', 'U0001F620', 'U0001F916']
d = dict(zip(data_classes, range(0,10)))
datas['Label']=datas['Label'].map(d, na_action='ignore')

labels=datas['Label']
articles=datas['comment']
print(len(labels))
print(len(articles))

from sklearn.model_selection import train_test_split

train_articles, validation_articles, train_labels, validation_labels = train_test_split(articles,labels, test_size = 0.20, random_state = 22)
print(train_articles.shape,validation_articles.shape)
print(train_labels.shape,validation_labels.shape)

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_articles)
word_index = tokenizer.word_index
dict(list(word_index.items())[0:10])

train_sequences = tokenizer.texts_to_sequences(train_articles)
print(train_sequences[10])

train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(len(train_sequences[0]))
print(len(train_padded[0]))

print(len(train_sequences[1]))
print(len(train_padded[1]))

print(len(train_sequences[10]))
print(len(train_padded[10]))
train_padded.shape

print(train_padded[10])

validation_sequences = tokenizer.texts_to_sequences(validation_articles)
validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

print(len(validation_sequences))
print(validation_padded.shape)

train_labels

training_label_seq = train_labels
validation_label_seq =validation_labels
#print(training_label_seq.shape)
#print(validation_label_seq.shape)

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_article(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])
print(decode_article(train_padded[10]))
print('---')
print(train_articles[10])

model = tf.keras.Sequential([
    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top
    tf.keras.layers.Embedding(vocab_size, embedding_dim),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),
#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    # use ReLU in place of tanh function since they are very good alternatives of each other.
    tf.keras.layers.Dense(embedding_dim, activation='relu'),
    # Add a Dense layer with 6 units and softmax activation.
    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.
    tf.keras.layers.Dense(10, activation='softmax')
])
model.summary()

from keras.utils import to_categorical
training_label_seq = to_categorical(training_label_seq, 10)
validation_label_seq = to_categorical(validation_label_seq, 10)

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
num_epochs = 20
history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

Y_pred = model.predict(validation_padded)

Y_pred = np.argmax(Y_pred, axis=1)
Y_test = np.argmax(validation_label_seq, axis=1)

print(classification_report(Y_test, Y_pred))